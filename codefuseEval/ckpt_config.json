{
  "CodeFuse-13B": {
    "v1": {
      "path": "/mnt/model/CodeFuse13B-evol-instruction-4K/",
      "processor_class": "codefuseEval.processor.codefuse13b.Codefuse13BProcessor",
      "tokenizer": {
        "truncation": true,
        "padding": true,
        "max_length": 600
      },
      "generation_config": {
        "greedy": {
          "do_sample": false,
          "num_beams": 1,
          "max_new_tokens": 512
        },
        "beams": {
          "do_sample": false,
          "num_beams": 5,
          "max_new_tokens": 600,
          "num_return_sequences": 1
        },
        "dosample": {
          "da_sample": true
        },
        "temperature": 0.2,
        "max_new_tokens": 600,
        "num_return_sequences": 1,
        "top_p": 0.9,
        "num_beams": 1,
        "do_sample": true
      },
      "batch_size": 1,
      "sample_num": 1,
      "decode_mode": "beams"
    }
  },
  "CodeFuse-CodeLlama-34B": {
    "v1": {
      "path": "/mnt/model/codellama-34b-python-mft-qlora-v1-step22000-merged",
      "processor_class": "codefuseEval.processor.codefusellama34b.CodeFuseCodeLlama34BProcessor",
      "tokenizer": {
        "truncation": true,
        "padding": true,
        "max_length": 512
      },
      "generation_config": {
        "greedy": {
          "do_sample": false,
          "num_beams": 1,
          "max_new_tokens": 512
        },
        "beams": {
          "do_sample": false,
          "num_beams": 5,
          "max_new_tokens": 600,
          "num_return_sequences": 1
        },
        "dosample": {
          "do_sample": true
        },
        "temperature": 0.2,
        "max_new_tokens": 512,
        "num_return_sequences": 1,
        "top_p": 0.9,
        "num_beams": 1,
        "do_sample": false
      },
      "batch_size": 1,
      "sample_num": 1,
      "decode_mode": "beams"
    }
  }
}